{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e93d514-3aa2-4a3e-9bcd-f2ffbc26a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 12:22:35.408087: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-14 12:22:35.412943: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 12:22:35.569371: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 12:22:36.252093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 12:22:38.354908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "# from keras import layers\n",
    "# from keras import ops\n",
    "# from keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e378d68-b21f-4f0a-b5bb-d0b17dca2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../midi_df_2199.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae016df2-28dc-4aaa-a6ea-bc2703445091",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 128\n",
    "BUFFER_SIZE = df.shape[0]\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a4e632-814a-475c-bc25-df4709337c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_to_max_len_list = max(df['tokenized_guitar'].apply(lambda x : len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe33f0-b35e-4f06-a64a-907ab4e8d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['tokenized_guitar'].apply(lambda x : len(x)).sort_values(ascending=False), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5befd926-1c0d-496e-a561-ee40437ba574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2199, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72cf91f6-121a-4989-bc00-8ec993503a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tokenized_guitar'] = df['tokenized_guitar'].apply(lambda x: np.asarray(x).astype(np.float32))\n",
    "# df['tokenized_drums'] = df['tokenized_drums'].apply(lambda x: np.asarray(x).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336286f8-3074-4aa2-bd7e-c392aba87ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tokenized_guitar'] = df['tokenized_guitar'].apply(lambda x: np.pad(x, pad_width=(0, (pad_to_max_len_list-len(x))), mode='constant', constant_values=0))\n",
    "# df['tokenized_drums'] = df['tokenized_drums'].apply(lambda x: np.pad(x, pad_width=(0, (pad_to_max_len_list-len(x))), mode='constant', constant_values=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b96461c-4bdc-4fc0-ae1c-991441b763c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df['tokenized_drums'][190])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f0733a08-4591-426f-a65e-d798392fb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "train_index = int(round((df.shape[0]*train_ratio)//BATCH_SIZE, 0)*BATCH_SIZE)\n",
    "test_size = int((((df.shape[0])-train_index)//BATCH_SIZE)*BATCH_SIZE)\n",
    "test_index = train_index + test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba86a8-3e2f-49ca-84ef-e589d925a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.iloc[:train_index].loc[:,['tokenized_guitar', 'tokenized_drums']]\n",
    "test_data = df.iloc[train_index:test_index].loc[:,['tokenized_guitar', 'tokenized_drums']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e682b-066a-40bd-82e7-cac34485c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['tokenized_guitar'] = train_data['tokenized_guitar'].apply(lambda x: tf.constant((x)))\n",
    "# train_data['tokenized_drums'] = train_data['tokenized_drums'].apply(lambda x: tf.constant((x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6796d6-4482-4f3b-9f19-1af1db832b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokenized_guitar'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f0be6-e97f-4c3e-aa51-c1503f737fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train = tf.data.Dataset.from_tensors((tf.ragged.constant(train_data['tokenized_guitar']).to_tensor(),\n",
    "#                                                           tf.ragged.constant(train_data['tokenized_drums']).to_tensor()))\n",
    "\n",
    "# ds_test = tf.data.Dataset.from_tensors((tf.ragged.constant(test_data['tokenized_guitar']).to_tensor(),\n",
    "#                                                           tf.ragged.constant(test_data['tokenized_drums']).to_tensor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809e947-5b85-4a82-b2ef-0e80ee823245",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "\n",
    "for guitar,drums in zip(tf.split(tf.ragged.constant(test_data['tokenized_guitar']).to_tensor(),test_size,axis=0),tf.split(tf.ragged.constant(test_data['tokenized_drums']).to_tensor(),test_size,axis=0)):\n",
    "    lst.append((guitar,drums))\n",
    "\n",
    "ds_test = tf.data.experimental.from_list(lst).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daa83b-2d19-41f7-a98b-170eb0fcbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "\n",
    "for guitar,drums in zip(tf.split(tf.ragged.constant(train_data['tokenized_guitar']).to_tensor(),train_index,axis=0),tf.split(tf.ragged.constant(train_data['tokenized_drums']).to_tensor(),train_index,axis=0)):\n",
    "    lst.append((guitar,drums))\n",
    "    \n",
    "ds_train = tf.data.experimental.from_list(lst).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb64f4-1071-419c-b787-b3a0d41016b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(guitar, drums):\n",
    "    guitar = guitar[:, :, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "\n",
    "    drums = drums[:, :, :(MAX_TOKENS+1)]\n",
    "    drums_input = drums[:, :, :-1]\n",
    "    drums_labels = drums[:, :, 1:]\n",
    "\n",
    "    return (tf.squeeze(guitar), tf.squeeze(drums_input)), tf.squeeze(drums_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff09e1-9a98-4308-b105-f5c3cdcfa398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .map(lambda *x: tf.nest.map_structure(lambda c: tf.ensure_shape(c, [BATCH_SIZE, MAX_TOKENS]), x))\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e790b2-6f7a-4b7e-bfb4-5241fafbd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation set batches.\n",
    "train_batches = make_batches(ds_train)\n",
    "val_batches = make_batches(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f9619-10ec-4b14-bda1-ebb2ba77bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98978a9-f01c-434b-adf0-458006dae86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "  \n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ca8d9-808d-4f54-90fb-4e5e80ab798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "# Check the shape.\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0baec-2b38-4d96-b122-67b24ea5ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n",
    "p = pos_encoding[1000]\n",
    "dots = tf.einsum('pd,d -> p', pos_encoding, p)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(dots)\n",
    "plt.ylim([0,1])\n",
    "plt.plot([950, 950, float('nan'), 1050, 1050],\n",
    "         [0,1,float('nan'),0,1], color='k', label='Zoom')\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(dots)\n",
    "plt.xlim([950, 1050])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6cff7-517a-4630-a278-e875beeaffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa4b34-e732-452a-8c48-8955191ef950",
   "metadata": {},
   "outputs": [],
   "source": [
    "guitar_vocab_size = 169499\n",
    "drums_vocab_size = 213357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a0348-58f9-4091-a7af-7a7189532d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_guitar = PositionalEmbedding(vocab_size=guitar_vocab_size, d_model=512)\n",
    "embed_drums = PositionalEmbedding(vocab_size=drums_vocab_size, d_model=512)\n",
    "\n",
    "guitar_emb = embed_guitar(guitar)\n",
    "drums_emb = embed_drums(drums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79307a48-6037-4c8a-a8e9-bb7f0831fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "drums_emb._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff00875-e739-4542-9124-423956acd021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479b6ec-5133-4752-b4ec-32812387362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "   \n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8c7bb-4a24-42fa-b1f7-22ce725f8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(guitar_emb.shape)\n",
    "print(drums_emb.shape)\n",
    "print(sample_ca(drums_emb, guitar_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4785e7a-438c-4cef-ada1-86a8821f4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c671dd-ed24-4572-a176-633b9ec1251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(guitar_emb.shape)\n",
    "print(sample_gsa(guitar_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332c3bc-1317-4f8e-a873-4b157cfd9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd239d-d91b-4e3c-a598-a964eb7f55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(drums_emb.shape)\n",
    "print(sample_csa(drums_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10f9416-1835-40f0-a78d-a649b0df805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out1 = sample_csa(drums_emb(drums[:, :3])) \n",
    "# out2 = sample_csa(drums_emb(drums))[:, :3]\n",
    "\n",
    "# tf.reduce_max(abs(out1 - out2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc11c6b-e1cf-475e-854d-b578e2de95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231ccba-f6a5-4d97-bbad-035774b2cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ffn = FeedForward(512, 2048)\n",
    "\n",
    "print(drums_emb.shape)\n",
    "print(sample_ffn(drums_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580e7f5-462d-4198-a065-ccf649f946cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a288e5-730d-411d-9ed3-c3b902b14eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "print(guitar_emb.shape)\n",
    "print(sample_encoder_layer(guitar_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685b3cc-989c-462b-b963-0518d85baa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26efb52b-299a-4098-8ac7-50c516de535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the encoder.\n",
    "sample_encoder = Encoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=guitar_vocab_size)\n",
    "\n",
    "sample_encoder_output = sample_encoder(guitar, training=False)\n",
    "\n",
    "# Print the shape.\n",
    "print(guitar.shape)\n",
    "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54928bfd-eae3-4eda-9f4d-e2830b7e2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "    \n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad344d-a513-4d01-8e41-58b507749859",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_decoder_layer_output = sample_decoder_layer(\n",
    "    x=drums_emb, context=guitar_emb)\n",
    "\n",
    "print(drums_emb.shape)\n",
    "print(guitar_emb.shape)\n",
    "print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e376c366-9006-48a0-8ad9-2dd26b9833f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296342a-c01e-409d-9558-790953f58cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=drums_vocab_size)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=drums,\n",
    "    context=guitar_emb)\n",
    "\n",
    "# Print the shapes.\n",
    "print(drums.shape)\n",
    "print(guitar_emb.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8e0ff-2655-4cfe-84f9-493419217f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433f5c2-6a71-42e1-b9a3-2f74447f5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d39fef-0269-405d-9a58-36b0ab45c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0170e-fd35-4d42-862c-9256a5cce7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=guitar_vocab_size,\n",
    "    target_vocab_size=drums_vocab_size,\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5cc27f-c37c-4797-b0ca-cc3c4d5a797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer((guitar, drums))\n",
    "\n",
    "print(drums.shape)\n",
    "print(guitar.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0f944-370c-47d5-996c-22d7175f0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
    "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895f5ad-35f5-4986-a135-fb3b7d1c2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535d76d-417e-4276-aa0e-6703466196d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873570a-8c2e-48b1-9fa7-76310cbcffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21eca6d-1987-4bc7-bae4-e98507d04c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d84f6-801a-42e1-9d35-f9a2f165ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f82ed5-f41b-4f25-ac3d-069cdcc8fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd611fc6-0e63-45d8-a563-3396be13a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(train_batches,\n",
    "                epochs=20,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7253921-2c9e-4772-87a0-2555516d1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(transformer, 'taxifare_pablopussell/model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b0b7c9-eac2-4377-bf11-1f84a8889fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = tf.saved_model.load('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "805879b6-ff27-4206-912f-a91559bbdece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, transformer):\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, guitar_tokens, max_length=MAX_TOKENS):\n",
    "    # The input is a tokenized guitar track.\n",
    "    guitar_tensor = tf.ragged.constant(guitar_tokens)\n",
    "    assert isinstance(guitar_tensor, tf.Tensor)\n",
    "    if len(guitar_tensor.shape) == 0:\n",
    "        guitar_tensor = guitar_tensor[tf.newaxis]\n",
    "\n",
    "    print(guitar_tensor)\n",
    "    encoder_input = guitar_tensor\n",
    "\n",
    "    # As the output is a tokenized drum track, initialize the output with the\n",
    "    # drum's `[START]` token.\n",
    "    start_token = tf.ragged.constant(1)\n",
    "    end_token = tf.ragged.constant(6)\n",
    "    start = start_token[tf.newaxis]\n",
    "    end = end_token[tf.newaxis]\n",
    "      \n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "    \n",
    "    output_array = output_array.write(0, start)\n",
    "    print(output_array)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions = self.transformer((encoder_input, output), training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`.\n",
    "    drum_tokenized = output[1]  # Shape: `()`.\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75d98839-ee6a-496c-ab8c-da8322ee46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d7d8bc-291b-4e21-b935-b4cdd9ea980d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'save_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/model_weights/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'save_weights'"
     ]
    }
   ],
   "source": [
    "transformer.save_weights('model/model_weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3550171-9e55-4763-9028-2b4e578bc110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2]['tokenized_guitar'][0] = 1\n",
    "df.iloc[2]['tokenized_guitar'][-1] = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c56e073-7bf9-4a1b-88e0-3cc4e118545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_ds = tf.data.experimental.from_list([df.iloc[2]['tokenized_guitar']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a465bf19-a76e-44c8-bf0b-7d1d456ef596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_ds.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7666afbd-497b-4339-b595-2ba2be0fc3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[     1      5      5      5      5      5      5      5      5      5\n",
      "      5      5      5      5      5 144952 143487 142160  80835 139724\n",
      " 138545 137405  78116 135241 134245 133173  75680 131321 130401 129565\n",
      "  73134 127847 127081 126419 125782 125169 124543 123968 123387 122920\n",
      " 122318  66933 121300 120821 120354  64953  64409 118985 118511 118092\n",
      " 117697 117337 116976  60888  60359 115933 115638 115350 115066 114779\n",
      " 114452 114228 113991      5      5      5      5      5      5      5\n",
      "      5 112345 112163  52760  52535 111674 111546 111427  51133 111193\n",
      " 111063  50140  49777 110753 110639 110554 110470 110372 110275 110207\n",
      " 110069 110028 109958 109899  46372 109783 109655  45546 109550 109504\n",
      " 109438  44489 109332 109295 109248  43557  43279 109098 109035 108990\n",
      "  42323 108895 108844  41607 108746      6], shape=(116,), dtype=int32)\n",
      "<tensorflow.python.util.tf_should_use.ShouldUseWrapper object at 0x7fb9f18f9960>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * (<tf.Tensor 'inputs:0' shape=(116,) dtype=int32>,\n <tf.Tensor 'inputs_1:0' shape=(1, 1) dtype=int32>)\n  Keyword arguments: {'training': False}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * (TensorSpec(shape=(None, 921), dtype=tf.int32, name='input_1'),\n TensorSpec(shape=(None, 921), dtype=tf.int32, name='input_2'))\n  Keyword arguments: {'training': False}\n\nOption 2:\n  Positional arguments (1 total):\n    * (TensorSpec(shape=(None, 921), dtype=tf.int32, name='inputs_0'),\n TensorSpec(shape=(None, 921), dtype=tf.int32, name='inputs_1'))\n  Keyword arguments: {'training': False}\n\nOption 3:\n  Positional arguments (1 total):\n    * (TensorSpec(shape=(None, 921), dtype=tf.int32, name='inputs_0'),\n TensorSpec(shape=(None, 921), dtype=tf.int32, name='inputs_1'))\n  Keyword arguments: {'training': True}\n\nOption 4:\n  Positional arguments (1 total):\n    * (TensorSpec(shape=(None, 921), dtype=tf.int32, name='input_1'),\n TensorSpec(shape=(None, 921), dtype=tf.int32, name='input_2'))\n  Keyword arguments: {'training': True}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m drum_tokens, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenized_guitar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 31\u001b[0m, in \u001b[0;36mTranslator.__call__\u001b[0;34m(self, guitar_tokens, max_length)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrange(max_length):\n\u001b[1;32m     30\u001b[0m   output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(output_array\u001b[38;5;241m.\u001b[39mstack())\n\u001b[0;32m---> 31\u001b[0m   predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;66;03m# Select the last token from the `seq_len` dimension.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m   predictions \u001b[38;5;241m=\u001b[39m predictions[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]  \u001b[38;5;66;03m# Shape `(batch_size, 1, vocab_size)`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:817\u001b[0m, in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_attribute\u001b[39m(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 817\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages/tensorflow/python/saved_model/function_deserialization.py:335\u001b[0m, in \u001b[0;36mrecreate_function.<locals>.restored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m   positional, keyword \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mstructured_input_signature\n\u001b[1;32m    332\u001b[0m   signature_descriptions\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    333\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOption \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword arguments: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    334\u001b[0m           index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, _pretty_format_positional(positional), keyword))\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find matching concrete function to call loaded from the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel. Got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_pretty_format_positional(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Expected these arguments to match one of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(saved_function\u001b[38;5;241m.\u001b[39mconcrete_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m option(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mjoin(signature_descriptions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * (<tf.Tensor 'inputs:0' shape=(116,) dtype=int32>,\n <tf.Tensor 'inputs_1:0' shape=(1, 1) dtype=int32>)\n  Keyword arguments: {'training': False}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * (TensorSpec(shape=(None, 921), dtype=tf.int32, name='input_1'),\n TensorSpec(shape=(None, 921), dtype=tf.int32, name='input_2'))\n  Keyword arguments: {'training': False}\n\nOption 2:\n  Positional arguments (1 total):\n    * (TensorSpec(shape=(None, 921), dtype=tf.int32, name='inputs_0'),\n TensorSpec(shape=(None, 921), dtype=tf.int32, name='inputs_1'))\n  Keyword arguments: {'training': False}\n\nOption 3:\n  Positional arguments (1 total):\n    * (TensorSpec(shape=(None, 921), dtype=tf.int32, name='inputs_0'),\n TensorSpec(shape=(None, 921), dtype=tf.int32, name='inputs_1'))\n  Keyword arguments: {'training': True}\n\nOption 4:\n  Positional arguments (1 total):\n    * (TensorSpec(shape=(None, 921), dtype=tf.int32, name='input_1'),\n TensorSpec(shape=(None, 921), dtype=tf.int32, name='input_2'))\n  Keyword arguments: {'training': True}"
     ]
    }
   ],
   "source": [
    "drum_tokens, attention_weights = translator(df.iloc[2]['tokenized_guitar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7030500-8a3e-4e20-8679-7597bc162ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
