{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028403f8-5044-4a60-b992-773ce0061f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: tokenizers in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: datasets in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: filelock in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: multiprocess in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: pandas in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/sergi_carapuig/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers tokenizers datasets\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b98304-4a3a-41b1-888f-592ad9fa5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4016\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 463\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"TristanBehrens/js-fakes-4bars\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e4a756-75c1-44d4-a6e9-a49b39be1ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'PIECE_START STYLE=JSFAKES GENRE=JSFAKES TRACK_START INST=0 BAR_START NOTE_ON=70 TIME_DELTA=4 NOTE_OFF=70 NOTE_ON=77 TIME_DELTA=4 NOTE_OFF=77 NOTE_ON=74 TIME_DELTA=4 NOTE_OFF=74 NOTE_ON=72 TIME_DELTA=2 NOTE_OFF=72 NOTE_ON=74 TIME_DELTA=2 NOTE_OFF=74 BAR_END BAR_START NOTE_ON=75 TIME_DELTA=4 NOTE_OFF=75 NOTE_ON=72 TIME_DELTA=8 NOTE_OFF=72 NOTE_ON=70 TIME_DELTA=4 NOTE_OFF=70 BAR_END BAR_START NOTE_ON=70 TIME_DELTA=4 NOTE_OFF=70 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 NOTE_ON=67 TIME_DELTA=4 NOTE_OFF=67 NOTE_ON=69 TIME_DELTA=4 NOTE_OFF=69 BAR_END BAR_START NOTE_ON=70 TIME_DELTA=2 NOTE_OFF=70 NOTE_ON=69 TIME_DELTA=2 NOTE_OFF=69 NOTE_ON=67 TIME_DELTA=8 NOTE_OFF=67 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 BAR_END TRACK_END TRACK_START INST=32 BAR_START NOTE_ON=58 TIME_DELTA=4 NOTE_OFF=58 NOTE_ON=57 TIME_DELTA=4 NOTE_OFF=57 NOTE_ON=58 TIME_DELTA=4 NOTE_OFF=58 NOTE_ON=53 TIME_DELTA=4 NOTE_OFF=53 BAR_END BAR_START NOTE_ON=51 TIME_DELTA=4 NOTE_OFF=51 NOTE_ON=53 TIME_DELTA=8 NOTE_OFF=53 NOTE_ON=46 TIME_DELTA=4 NOTE_OFF=46 BAR_END BAR_START NOTE_ON=58 TIME_DELTA=4 NOTE_OFF=58 NOTE_ON=57 TIME_DELTA=4 NOTE_OFF=57 NOTE_ON=55 TIME_DELTA=4 NOTE_OFF=55 NOTE_ON=53 TIME_DELTA=2 NOTE_OFF=53 NOTE_ON=51 TIME_DELTA=2 NOTE_OFF=51 BAR_END BAR_START NOTE_ON=50 TIME_DELTA=2 NOTE_OFF=50 NOTE_ON=48 TIME_DELTA=2 NOTE_OFF=48 NOTE_ON=46 TIME_DELTA=4 NOTE_OFF=46 NOTE_ON=48 TIME_DELTA=4 NOTE_OFF=48 NOTE_ON=41 TIME_DELTA=4 NOTE_OFF=41 BAR_END TRACK_END TRACK_START INST=48 BAR_START NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 BAR_END BAR_START NOTE_ON=67 TIME_DELTA=4 NOTE_OFF=67 NOTE_ON=65 TIME_DELTA=8 NOTE_OFF=65 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 BAR_END BAR_START NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 NOTE_ON=62 TIME_DELTA=2 NOTE_OFF=62 NOTE_ON=64 TIME_DELTA=2 NOTE_OFF=64 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 BAR_END BAR_START NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 NOTE_ON=65 TIME_DELTA=4 NOTE_OFF=65 NOTE_ON=64 TIME_DELTA=4 NOTE_OFF=64 NOTE_ON=60 TIME_DELTA=4 NOTE_OFF=60 BAR_END TRACK_END TRACK_START INST=24 BAR_START NOTE_ON=62 TIME_DELTA=4 NOTE_OFF=62 NOTE_ON=60 TIME_DELTA=4 NOTE_OFF=60 NOTE_ON=58 TIME_DELTA=4 NOTE_OFF=58 NOTE_ON=57 TIME_DELTA=4 NOTE_OFF=57 BAR_END BAR_START NOTE_ON=58 TIME_DELTA=4 NOTE_OFF=58 NOTE_ON=58 TIME_DELTA=4 NOTE_OFF=58 NOTE_ON=57 TIME_DELTA=4 NOTE_OFF=57 NOTE_ON=62 TIME_DELTA=4 NOTE_OFF=62 BAR_END BAR_START NOTE_ON=62 TIME_DELTA=4 NOTE_OFF=62 NOTE_ON=60 TIME_DELTA=4 NOTE_OFF=60 NOTE_ON=58 TIME_DELTA=4 NOTE_OFF=58 NOTE_ON=60 TIME_DELTA=4 NOTE_OFF=60 BAR_END BAR_START NOTE_ON=58 TIME_DELTA=2 NOTE_OFF=58 NOTE_ON=60 TIME_DELTA=2 NOTE_OFF=60 NOTE_ON=62 TIME_DELTA=4 NOTE_OFF=62 NOTE_ON=60 TIME_DELTA=2 NOTE_OFF=60 NOTE_ON=58 TIME_DELTA=2 NOTE_OFF=58 NOTE_ON=57 TIME_DELTA=4 NOTE_OFF=57 BAR_END TRACK_END PIECE_END'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3e5e96-7e61-4250-b425-9b4dca16fd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2868"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c25fed-3a7c-4276-ba85-fbcf7e0bec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "trainer = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "tokenizer.train_from_iterator(training_corpus, trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84cb1aff-a44d-4cec-825e-d417a86eb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def train_and_save_tokenizer(training_dataset, tokenizer_file=\"tokenizer.json\"):\n",
    "    # Inicializar un Tokenizer con modelo WordLevel\n",
    "    tokenizer_model = WordLevel(unk_token=\"[UNK]\")\n",
    "    \n",
    "    # Configurar el pre-tokenizador para dividir por espacios en blanco\n",
    "    pre_tokenizer = WhitespaceSplit()\n",
    "    \n",
    "    # Entrenador para el modelo WordLevel\n",
    "    trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    \n",
    "    # Crear un Tokenizer\n",
    "    tokenizer = Tokenizer(tokenizer_model)\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "    \n",
    "    # Entrenar el Tokenizer desde un iterador sobre el corpus de entrenamiento\n",
    "    for start_idx in range(0, len(training_dataset), 1000):\n",
    "        samples = training_dataset[start_idx : start_idx + 1000]\n",
    "        tokenizer.train_from_iterator(samples[\"text\"], trainer=trainer)\n",
    "    \n",
    "    # Guardar el Tokenizer\n",
    "    tokenizer.save(tokenizer_file)\n",
    "    \n",
    "    # Cargar el Tokenizer pre-entrenado\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "    \n",
    "    # Agregar token especial de padding\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38edb6db-4da6-418a-b0ae-c18164f48605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOTE_OFF=48': 58,\n",
       " 'NOTE_ON=64': 21,\n",
       " 'NOTE_ON=81': 116,\n",
       " 'NOTE_ON=44': 87,\n",
       " 'NOTE_OFF=60': 17,\n",
       " '[PAD]': 3,\n",
       " 'NOTE_OFF=78': 93,\n",
       " 'NOTE_ON=56': 43,\n",
       " 'NOTE_ON=42': 98,\n",
       " 'NOTE_ON=57': 41,\n",
       " 'NOTE_OFF=79': 99,\n",
       " 'NOTE_OFF=41': 95,\n",
       " 'BAR_START': 8,\n",
       " '[MASK]': 4,\n",
       " 'BAR_END': 7,\n",
       " 'NOTE_ON=63': 23,\n",
       " 'NOTE_ON=47': 67,\n",
       " '[UNK]': 0,\n",
       " 'TIME_DELTA=6': 85,\n",
       " 'NOTE_OFF=55': 34,\n",
       " 'NOTE_ON=52': 53,\n",
       " 'NOTE_OFF=66': 24,\n",
       " 'NOTE_ON=40': 102,\n",
       " 'NOTE_ON=72': 49,\n",
       " 'NOTE_ON=54': 51,\n",
       " 'TIME_DELTA=10': 118,\n",
       " 'NOTE_OFF=42': 97,\n",
       " 'NOTE_OFF=59': 38,\n",
       " 'NOTE_OFF=71': 46,\n",
       " 'NOTE_OFF=70': 36,\n",
       " 'TIME_DELTA=3': 108,\n",
       " 'NOTE_ON=65': 14,\n",
       " 'NOTE_OFF=64': 20,\n",
       " 'NOTE_ON=71': 47,\n",
       " 'NOTE_ON=37': 112,\n",
       " 'NOTE_OFF=58': 32,\n",
       " 'NOTE_ON=75': 69,\n",
       " 'TIME_DELTA=2': 6,\n",
       " 'NOTE_ON=59': 39,\n",
       " 'GENRE=JSFAKES': 72,\n",
       " 'TRACK_END': 9,\n",
       " 'NOTE_OFF=68': 28,\n",
       " 'NOTE_ON=39': 105,\n",
       " 'NOTE_OFF=36': 113,\n",
       " 'NOTE_OFF=63': 22,\n",
       " 'NOTE_OFF=54': 50,\n",
       " 'INST=24': 74,\n",
       " 'NOTE_OFF=76': 83,\n",
       " 'NOTE_ON=55': 35,\n",
       " 'NOTE_ON=51': 57,\n",
       " 'NOTE_ON=61': 31,\n",
       " '[CLS]': 1,\n",
       " 'STYLE=JSFAKES': 79,\n",
       " '[SEP]': 2,\n",
       " 'NOTE_ON=76': 84,\n",
       " 'TIME_DELTA=16': 103,\n",
       " 'NOTE_ON=50': 55,\n",
       " 'NOTE_ON=74': 63,\n",
       " 'INST=48': 76,\n",
       " 'TIME_DELTA=12': 88,\n",
       " 'NOTE_OFF=77': 91,\n",
       " 'NOTE_ON=68': 29,\n",
       " 'NOTE_OFF=73': 60,\n",
       " 'NOTE_ON=70': 37,\n",
       " 'NOTE_ON=36': 114,\n",
       " 'TIME_DELTA=1': 80,\n",
       " 'NOTE_ON=78': 94,\n",
       " 'NOTE_ON=67': 12,\n",
       " 'NOTE_ON=69': 27,\n",
       " 'NOTE_ON=73': 61,\n",
       " 'NOTE_ON=58': 33,\n",
       " 'NOTE_ON=60': 18,\n",
       " 'NOTE_ON=49': 65,\n",
       " 'PIECE_END': 77,\n",
       " 'PIECE_START': 78,\n",
       " 'NOTE_ON=79': 100,\n",
       " 'NOTE_OFF=80': 106,\n",
       " 'NOTE_OFF=38': 109,\n",
       " 'NOTE_OFF=43': 89,\n",
       " 'NOTE_ON=45': 82,\n",
       " 'NOTE_ON=66': 25,\n",
       " 'NOTE_OFF=39': 104,\n",
       " 'NOTE_OFF=52': 52,\n",
       " 'NOTE_ON=77': 92,\n",
       " 'NOTE_OFF=45': 81,\n",
       " 'NOTE_OFF=81': 115,\n",
       " 'NOTE_ON=62': 16,\n",
       " 'INST=0': 73,\n",
       " 'NOTE_OFF=74': 62,\n",
       " 'NOTE_ON=43': 90,\n",
       " 'TIME_DELTA=7': 117,\n",
       " 'NOTE_OFF=62': 15,\n",
       " 'NOTE_OFF=46': 70,\n",
       " 'NOTE_ON=41': 96,\n",
       " 'NOTE_ON=53': 45,\n",
       " 'NOTE_OFF=49': 64,\n",
       " 'NOTE_OFF=47': 66,\n",
       " 'NOTE_ON=38': 110,\n",
       " 'NOTE_OFF=69': 26,\n",
       " 'NOTE_OFF=56': 42,\n",
       " 'NOTE_OFF=40': 101,\n",
       " 'NOTE_OFF=50': 54,\n",
       " 'INST=32': 75,\n",
       " 'NOTE_OFF=67': 11,\n",
       " 'NOTE_OFF=44': 86,\n",
       " 'NOTE_ON=80': 107,\n",
       " 'NOTE_ON=46': 71,\n",
       " 'TRACK_START': 10,\n",
       " 'NOTE_ON=48': 59,\n",
       " 'NOTE_OFF=75': 68,\n",
       " 'NOTE_OFF=37': 111,\n",
       " 'TIME_DELTA=4': 5,\n",
       " 'NOTE_OFF=57': 40,\n",
       " 'NOTE_OFF=61': 30,\n",
       " 'NOTE_OFF=72': 48,\n",
       " 'NOTE_OFF=65': 13,\n",
       " 'TIME_DELTA=8': 19,\n",
       " 'NOTE_OFF=53': 44,\n",
       " 'NOTE_OFF=51': 56}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47013336-d45b-4641-8d4b-550dbd959f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d9df29-713d-4cba-a03e-1434e25fea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"training_name\": \"jsfakes_mmmtrack_4bars_full\",\n",
    "    \"dataset_name\": \"jsfakes_mmmtrack_4bars_full\",\n",
    "    \"model\": {\n",
    "        \"n_ctx\": 512,\n",
    "        \"n_embd\": 512,\n",
    "        \"n_head\": 8,\n",
    "        \"n_layer\": 6,\n",
    "        \"n_positions\": 512\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"pad_length\": 512,\n",
    "        \"shuffle_buffer_size\": 10000,\n",
    "        \"batch_size\": 10,\n",
    "        \"num_train_epochs\": 20,\n",
    "        \"save_steps\": 300,\n",
    "        \"save_total_limit\": 20\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdafd5dd-9d62-4e3b-8de6-ebb5bb984645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokenized_example = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=training_config[\"model\"][\"n_positions\"],\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenized_example[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# Check a sample.\n",
    "tokenized = tokenize_function(raw_datasets[\"train\"][0])\n",
    "assert list(tokenized.keys()) == [\"input_ids\"], list(tokenized.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a9acef-e976-43ad-9dc3-e16723d87803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271352bfa032436ebb899c136ad49bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe9d8f91ac342069d87c94f3a3795a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [78, 79, 72, 10, 73, 8, 37, 5, 36, 92, 5, 91, 63, 5, 62, 49, 6, 48, 63, 6, 62, 7, 8, 69, 5, 68, 49, 19, 48, 37, 5, 36, 7, 8, 37, 5, 36, 14, 5, 13, 12, 5, 11, 27, 5, 26, 7, 8, 37, 6, 36, 27, 6, 26, 12, 19, 11, 14, 5, 13, 7, 9, 10, 75, 8, 33, 5, 32, 41, 5, 40, 33, 5, 32, 45, 5, 44, 7, 8, 57, 5, 56, 45, 19, 44, 71, 5, 70, 7, 8, 33, 5, 32, 41, 5, 40, 35, 5, 34, 45, 6, 44, 57, 6, 56, 7, 8, 55, 6, 54, 59, 6, 58, 71, 5, 70, 59, 5, 58, 96, 5, 95, 7, 9, 10, 76, 8, 14, 5, 13, 14, 5, 13, 14, 5, 13, 14, 5, 13, 7, 8, 12, 5, 11, 14, 19, 13, 14, 5, 13, 7, 8, 14, 5, 13, 14, 5, 13, 16, 6, 15, 21, 6, 20, 14, 5, 13, 7, 8, 14, 5, 13, 14, 5, 13, 21, 5, 20, 18, 5, 17, 7, 9, 10, 74, 8, 16, 5, 15, 18, 5, 17, 33, 5, 32, 41, 5, 40, 7, 8, 33, 5, 32, 33, 5, 32, 41, 5, 40, 16, 5, 15, 7, 8, 16, 5, 15, 18, 5, 17, 33, 5, 32, 18, 5, 17, 7, 8, 33, 6, 32, 18, 6, 17, 16, 5, 15, 18, 6, 17, 33, 6, 32, 41, 5, 40, 7, 9, 77]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "\n",
    "# Check a sample.\n",
    "tokenized = tokenized_datasets[\"train\"][0]\n",
    "assert list(tokenized.keys()) == [\"input_ids\"], list(tokenized.keys())\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "736f237f-1562-4142-93f1-edd653bd789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 13:57:55.947755: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-06 13:57:56.016915: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-06 13:57:56.395725: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-06 13:57:56.395843: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-06 13:57:56.462981: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-06 13:57:56.594651: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-06 13:57:56.599869: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 13:57:58.280515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "835869e0-729b-43ba-88f0-806b206b5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19845bef-3202-4175-8976-0757cc52ee2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nGPT2LMHeadModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFGPT2LMHeadModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Config, GPT2LMHeadModel\n\u001b[1;32m      3\u001b[0m model_config \u001b[38;5;241m=\u001b[39m GPT2Config(\n\u001b[1;32m      4\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m      5\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     n_positions\u001b[38;5;241m=\u001b[39mtraining_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_positions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages/transformers/utils/dummy_pt_objects.py:3932\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3931\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 3932\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BandIt/lib/python3.10/site-packages/transformers/utils/import_utils.py:1309\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[0;31mImportError\u001b[0m: \nGPT2LMHeadModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFGPT2LMHeadModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    n_ctx=training_config[\"model\"][\"n_ctx\"],\n",
    "    n_embd=training_config[\"model\"][\"n_embd\"],\n",
    "    n_head=training_config[\"model\"][\"n_head\"],\n",
    "    n_layer=training_config[\"model\"][\"n_layer\"],\n",
    "    n_positions=training_config[\"model\"][\"n_positions\"],\n",
    ")\n",
    "model = GPT2LMHeadModel(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15feeba-5b99-495a-8c2f-5fa9b149cebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
